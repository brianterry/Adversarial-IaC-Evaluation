{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ® Single Game: Red Team vs Blue Team\n",
        "\n",
        "This notebook walks through running a single adversarial game where:\n",
        "- **Red Team** generates vulnerable IaC code with hidden security flaws\n",
        "- **Blue Team** attempts to detect all vulnerabilities\n",
        "- **Judge** scores the match using precision, recall, and F1 metrics\n",
        "\n",
        "## Prerequisites\n",
        "- AWS credentials configured (for Bedrock access)\n",
        "- `.env` file with `AWS_REGION` set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add project root to path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Navigate to project root\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Load environment variables\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(project_root / '.env')\n",
        "\n",
        "# Core imports\n",
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Project imports\n",
        "from src.agents.red_team_agent import create_red_team_agent, Difficulty\n",
        "from src.agents.blue_team_agent import create_blue_team_agent, DetectionMode\n",
        "from src.agents.judge_agent import JudgeAgent, score_results_to_dict\n",
        "from src.game.scenarios import ScenarioGenerator\n",
        "\n",
        "print(f\"âœ“ Project root: {project_root}\")\n",
        "print(\"âœ“ All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure the Game\n",
        "\n",
        "Modify these settings to customize the adversarial match:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ğŸ® GAME CONFIGURATION - Modify these!\n",
        "# ============================================\n",
        "\n",
        "# The scenario describes what infrastructure to create\n",
        "SCENARIO = \"Create an S3 bucket for storing healthcare PHI data with proper security controls\"\n",
        "\n",
        "# Difficulty affects how subtle the vulnerabilities are\n",
        "# Options: \"easy\", \"medium\", \"hard\"\n",
        "DIFFICULTY = \"medium\"\n",
        "\n",
        "# Models to use (must be available in your AWS Bedrock region)\n",
        "RED_TEAM_MODEL = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"   # Attacker\n",
        "BLUE_TEAM_MODEL = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"  # Defender\n",
        "\n",
        "# IaC settings\n",
        "LANGUAGE = \"terraform\"      # or \"cloudformation\"\n",
        "CLOUD_PROVIDER = \"aws\"      # or \"azure\", \"gcp\"\n",
        "\n",
        "# AWS region for Bedrock\n",
        "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
        "\n",
        "print(f\"ğŸ“‹ Scenario: {SCENARIO}\")\n",
        "print(f\"ğŸ¯ Difficulty: {DIFFICULTY}\")\n",
        "print(f\"ğŸ”´ Red Team Model: {RED_TEAM_MODEL}\")\n",
        "print(f\"ğŸ”µ Blue Team Model: {BLUE_TEAM_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ğŸ”´ Red Team Attack\n",
        "\n",
        "The Red Team generates IaC code that appears legitimate but contains hidden vulnerabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Red Team agent\n",
        "red_agent = create_red_team_agent(\n",
        "    model_id=RED_TEAM_MODEL,\n",
        "    region=AWS_REGION,\n",
        "    difficulty=Difficulty[DIFFICULTY.upper()],\n",
        "    language=LANGUAGE,\n",
        "    cloud_provider=CLOUD_PROVIDER\n",
        ")\n",
        "\n",
        "print(\"ğŸ”´ Red Team agent created\")\n",
        "print(f\"   Difficulty: {DIFFICULTY}\")\n",
        "print(f\"   Language: {LANGUAGE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate vulnerable IaC\n",
        "print(\"ğŸ”´ Red Team generating vulnerable code...\")\n",
        "red_result = await red_agent.generate(SCENARIO)\n",
        "\n",
        "print(f\"\\nâœ“ Generated {len(red_result.files)} file(s)\")\n",
        "print(f\"âœ“ Injected {len(red_result.vulnerabilities)} vulnerabilities\")\n",
        "\n",
        "# Show the generated code\n",
        "for filename, content in red_result.files.items():\n",
        "    print(f\"\\nğŸ“„ {filename}:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(content[:1500] + \"...\" if len(content) > 1500 else content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show what Red Team secretly injected (ground truth)\n",
        "print(\"ğŸ”´ RED TEAM'S SECRET VULNERABILITIES (Ground Truth):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, vuln in enumerate(red_result.vulnerabilities, 1):\n",
        "    print(f\"\\n{i}. {vuln.get('title', 'Unknown')}\")\n",
        "    print(f\"   Type: {vuln.get('vulnerability_type', 'N/A')}\")\n",
        "    print(f\"   Resource: {vuln.get('resource_name', 'N/A')}\")\n",
        "    print(f\"   Severity: {vuln.get('severity', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ğŸ”µ Blue Team Defense\n",
        "\n",
        "The Blue Team analyzes the code and attempts to find all vulnerabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Blue Team agent\n",
        "blue_agent = create_blue_team_agent(\n",
        "    model_id=BLUE_TEAM_MODEL,\n",
        "    region=AWS_REGION,\n",
        "    mode=DetectionMode.LLM_ONLY  # Options: LLM_ONLY, TOOLS_ONLY, HYBRID\n",
        ")\n",
        "\n",
        "print(\"ğŸ”µ Blue Team agent created\")\n",
        "print(f\"   Mode: LLM_ONLY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Blue Team analyzes the code\n",
        "print(\"ğŸ”µ Blue Team analyzing code...\")\n",
        "blue_result = await blue_agent.analyze(red_result.files)\n",
        "\n",
        "print(f\"\\nâœ“ Blue Team found {len(blue_result.findings)} potential vulnerabilities\")\n",
        "\n",
        "# Show Blue Team's findings\n",
        "print(\"\\nğŸ”µ BLUE TEAM'S FINDINGS:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, finding in enumerate(blue_result.findings, 1):\n",
        "    print(f\"\\n{i}. {finding.get('title', 'Unknown')}\")\n",
        "    print(f\"   Resource: {finding.get('resource_name', 'N/A')}\")\n",
        "    print(f\"   Severity: {finding.get('severity', 'N/A')}\")\n",
        "    print(f\"   Evidence: {finding.get('evidence', 'N/A')[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. âš–ï¸ Judge Scoring\n",
        "\n",
        "The Judge compares Red Team's ground truth against Blue Team's findings to calculate metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Judge scores the match\n",
        "judge = JudgeAgent()\n",
        "scoring = judge.score(\n",
        "    red_vulnerabilities=red_result.vulnerabilities,\n",
        "    blue_findings=blue_result.findings\n",
        ")\n",
        "\n",
        "# Convert to dict for display\n",
        "results = score_results_to_dict(scoring)\n",
        "\n",
        "print(\"âš–ï¸ JUDGE'S SCORING RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nğŸ“Š METRICS:\")\n",
        "print(f\"   Precision:    {results['metrics']['precision']:.1%}\")\n",
        "print(f\"   Recall:       {results['metrics']['recall']:.1%}\")\n",
        "print(f\"   F1 Score:     {results['metrics']['f1_score']:.1%}\")\n",
        "print(f\"   Evasion Rate: {results['metrics']['evasion_rate']:.1%}\")\n",
        "\n",
        "print(f\"\\nğŸ“ˆ COUNTS:\")\n",
        "print(f\"   Red Team vulnerabilities: {results['counts']['total_red_vulns']}\")\n",
        "print(f\"   Blue Team findings:       {results['counts']['total_blue_findings']}\")\n",
        "print(f\"   True Positives:           {results['counts']['true_positives']}\")\n",
        "print(f\"   False Positives:          {results['counts']['false_positives']}\")\n",
        "print(f\"   False Negatives:          {results['counts']['false_negatives']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show detailed match analysis\n",
        "print(\"\\nğŸ” DETAILED MATCH ANALYSIS:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if results.get('matches'):\n",
        "    for match in results['matches']:\n",
        "        print(f\"\\nâœ“ {match['match_type'].upper()} MATCH\")\n",
        "        print(f\"  Red: {match.get('red_title', 'N/A')}\")\n",
        "        print(f\"  Blue: {match.get('blue_title', 'N/A')}\")\n",
        "        print(f\"  Confidence: {match.get('confidence', 0):.0%}\")\n",
        "\n",
        "if results['details'].get('false_negative_ids'):\n",
        "    print(f\"\\nâŒ EVADED DETECTION ({len(results['details']['false_negative_ids'])}):\")\n",
        "    for fn_id in results['details']['false_negative_ids']:\n",
        "        print(f\"   - {fn_id}\")\n",
        "\n",
        "if results['details'].get('false_positive_ids'):\n",
        "    print(f\"\\nâš ï¸ FALSE ALARMS ({len(results['details']['false_positive_ids'])}):\")\n",
        "    for fp_id in results['details']['false_positive_ids']:\n",
        "        print(f\"   - {fp_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. ğŸ’¾ Save Results (Optional)\n",
        "\n",
        "Save the game results for later analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save results to output directory\n",
        "output_dir = project_root / \"output\" / f\"game_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save code files\n",
        "code_dir = output_dir / \"code\"\n",
        "code_dir.mkdir(exist_ok=True)\n",
        "for filename, content in red_result.files.items():\n",
        "    (code_dir / filename).write_text(content)\n",
        "\n",
        "# Save manifests and results\n",
        "with open(output_dir / \"red_manifest.json\", \"w\") as f:\n",
        "    json.dump({\"vulnerabilities\": red_result.vulnerabilities}, f, indent=2)\n",
        "\n",
        "with open(output_dir / \"blue_findings.json\", \"w\") as f:\n",
        "    json.dump({\"findings\": blue_result.findings}, f, indent=2)\n",
        "\n",
        "with open(output_dir / \"scoring.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"âœ“ Results saved to: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ğŸ¯ Summary\n",
        "\n",
        "This notebook demonstrated a single adversarial game. Key takeaways:\n",
        "\n",
        "- **Precision** tells you how accurate Blue Team's detections were (fewer false alarms = higher precision)\n",
        "- **Recall** tells you how complete the detection was (fewer missed vulns = higher recall)  \n",
        "- **F1 Score** balances both metrics\n",
        "- **Evasion Rate** shows how successful Red Team was at hiding vulnerabilities\n",
        "\n",
        "### Next Steps\n",
        "- Try different difficulty levels (`easy`, `medium`, `hard`)\n",
        "- Compare different models (Haiku vs Sonnet)\n",
        "- Use `HYBRID` mode to combine LLM with static analysis tools\n",
        "- Run batch experiments with `02_run_experiment.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final summary\n",
        "print(\"=\" * 60)\n",
        "print(\"ğŸ® GAME COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nğŸ”´ Red Team injected {len(red_result.vulnerabilities)} vulnerabilities\")\n",
        "print(f\"ğŸ”µ Blue Team found {len(blue_result.findings)} issues\")\n",
        "print(f\"âš–ï¸  F1 Score: {results['metrics']['f1_score']:.1%}\")\n",
        "\n",
        "if results['metrics']['f1_score'] >= 0.7:\n",
        "    print(\"\\nğŸ† Blue Team wins! Strong detection performance.\")\n",
        "elif results['metrics']['evasion_rate'] >= 0.3:\n",
        "    print(\"\\nğŸ† Red Team wins! Successfully evaded detection.\")\n",
        "else:\n",
        "    print(\"\\nğŸ¤ Close match! Both teams performed well.\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
