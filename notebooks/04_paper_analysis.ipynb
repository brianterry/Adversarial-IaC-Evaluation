{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“Š Paper Analysis: Adversarial IaC Evaluation\n",
        "\n",
        "This notebook analyzes experiment results for the research paper.\n",
        "\n",
        "## Experiments\n",
        "- **E3**: Novel vs Database Vulnerabilities (key finding)\n",
        "- **E1**: Model Comparison\n",
        "- **E4**: Difficulty Scaling\n",
        "- **E2**: Multi-Agent Ablation\n",
        "\n",
        "## Setup\n",
        "Copy experiment result CSVs from EC2:\n",
        "```bash\n",
        "mkdir -p notebooks/data\n",
        "scp ec2-user@your-ec2:~/experiment/Adversarial-IaC-Evaluation/experiments/results/*/results.csv notebooks/data/\n",
        "```\n",
        "Or point directly to `experiments/results/` if running on EC2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from scipy import stats\n",
        "import json, warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('Set2')\n",
        "plt.rcParams.update({\n",
        "    'figure.figsize': [10, 6], 'font.size': 12,\n",
        "    'axes.titlesize': 14, 'figure.dpi': 150,\n",
        "    'savefig.dpi': 300, 'savefig.bbox': 'tight',\n",
        "})\n",
        "\n",
        "project_root = Path.cwd().parent\n",
        "figures_dir = project_root / 'notebooks' / 'figures'\n",
        "figures_dir.mkdir(exist_ok=True)\n",
        "exp_dir = project_root / 'experiments' / 'results'\n",
        "\n",
        "def load_exp(path, name):\n",
        "    df = pd.read_csv(path); df['experiment'] = name\n",
        "    for col in ['red_model', 'blue_model']:\n",
        "        if col in df.columns:\n",
        "            df[col + '_short'] = (df[col]\n",
        "                .str.replace(r'.*sonnet.*', 'Sonnet', regex=True)\n",
        "                .str.replace(r'.*haiku.*', 'Haiku', regex=True)\n",
        "                .str.replace(r'.*nova-pro.*', 'Nova Pro', regex=True)\n",
        "                .str.replace(r'.*nova-lite.*', 'Nova Lite', regex=True)\n",
        "                .str.replace(r'.*llama.*70b.*', 'Llama 70B', regex=True))\n",
        "    return df\n",
        "\n",
        "# Auto-load all experiments\n",
        "experiments = {}\n",
        "for d in sorted(exp_dir.iterdir()):\n",
        "    csv = d / 'results.csv'\n",
        "    if csv.exists():\n",
        "        cfg = json.loads((d/'config.json').read_text()) if (d/'config.json').exists() else {}\n",
        "        name = cfg.get('name', d.name)\n",
        "        experiments[name] = load_exp(csv, name)\n",
        "        print(f'  âœ“ {name}: {len(experiments[name])} games')\n",
        "\n",
        "total = sum(len(df) for df in experiments.values())\n",
        "print(f'\\nTotal: {len(experiments)} experiments, {total} games')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E3: Novel vs Database Vulnerabilities\n",
        "**RQ**: Do LLMs demonstrate genuine security reasoning, or just pattern matching?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find E3 data\n",
        "e3 = next((df for k, df in experiments.items() if any(x in k.lower() for x in ['novel','e3','database'])), pd.DataFrame())\n",
        "print(f'E3: {len(e3)} games') if len(e3) > 0 else print('E3 not found')\n",
        "\n",
        "if len(e3) > 0:\n",
        "    # Summary table\n",
        "    display(e3.groupby('condition').agg(\n",
        "        F1=('f1_score', ['mean','std','count']),\n",
        "        Recall=('recall', ['mean','std']),\n",
        "        Precision=('precision', ['mean','std']),\n",
        "        Evasion=('evasion_rate', ['mean','std']),\n",
        "    ).round(3))\n",
        "    \n",
        "    # T-tests\n",
        "    db = e3[e3['condition']=='database']\n",
        "    novel = e3[e3['condition']=='novel']\n",
        "    \n",
        "    if len(db) > 0 and len(novel) > 0:\n",
        "        print('\\n=== STATISTICAL TESTS ===')\n",
        "        for metric, label in [('recall','Recall'), ('f1_score','F1'), ('evasion_rate','Evasion')]:\n",
        "            t, p = stats.ttest_ind(db[metric], novel[metric], equal_var=False)\n",
        "            d = abs(db[metric].mean()-novel[metric].mean()) / np.sqrt(\n",
        "                ((len(db)-1)*db[metric].std()**2 + (len(novel)-1)*novel[metric].std()**2) / (len(db)+len(novel)-2))\n",
        "            stars = '***' if p<.001 else '**' if p<.01 else '*' if p<.05 else 'ns'\n",
        "            print(f'\\n{label}: DB={db[metric].mean():.1%} vs Novel={novel[metric].mean():.1%}')\n",
        "            print(f'  t={t:.3f}, p={p:.4f} {stars}, d={d:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure: E3 Novel vs Database\n",
        "if len(e3) > 0:\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
        "    order = ['database', 'novel', 'mixed']\n",
        "    colors = {'database': '#2ecc71', 'novel': '#e74c3c', 'mixed': '#f39c12'}\n",
        "    \n",
        "    for ax, metric, title in zip(axes,\n",
        "        ['recall', 'f1_score', 'evasion_rate'],\n",
        "        ['Recall', 'F1 Score', 'Evasion Rate']):\n",
        "        sns.boxplot(data=e3, x='condition', y=metric, order=order, palette=colors, ax=ax, width=0.6)\n",
        "        sns.stripplot(data=e3, x='condition', y=metric, order=order, color='black', alpha=0.3, size=3, ax=ax)\n",
        "        ax.set_title(title, fontweight='bold')\n",
        "        ax.set_xlabel('Vulnerability Source')\n",
        "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
        "    \n",
        "    plt.suptitle('E3: Novel vs Database Vulnerability Detection', fontsize=15, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'e3_novel_vs_database.pdf')\n",
        "    plt.savefig(figures_dir / 'e3_novel_vs_database.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E1: Model Comparison\n",
        "**RQ**: How do different LLMs compare at IaC security detection?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find E1 data\n",
        "e1 = next((df for k, df in experiments.items() if any(x in k.lower() for x in ['model','e1','comparison'])), pd.DataFrame())\n",
        "print(f'E1: {len(e1)} games') if len(e1) > 0 else print('E1 not found')\n",
        "\n",
        "if len(e1) > 0:\n",
        "    summary = e1.groupby('red_model_short').agg(\n",
        "        F1=('f1_score', ['mean','std','count']),\n",
        "        Recall=('recall', ['mean','std']),\n",
        "        Precision=('precision', ['mean','std']),\n",
        "        Evasion=('evasion_rate', ['mean','std']),\n",
        "    ).round(3).sort_values(('F1','mean'), ascending=False)\n",
        "    display(summary)\n",
        "    \n",
        "    # ANOVA\n",
        "    groups = [g['f1_score'].values for _, g in e1.groupby('red_model_short')]\n",
        "    f_stat, p_val = stats.f_oneway(*groups)\n",
        "    ss_b = sum(len(g)*(g.mean()-e1['f1_score'].mean())**2 for _,g in e1.groupby('red_model_short')['f1_score'])\n",
        "    ss_t = sum((e1['f1_score']-e1['f1_score'].mean())**2)\n",
        "    print(f'\\nANOVA: F={f_stat:.3f}, p={p_val:.4f}, Î·Â²={ss_b/ss_t:.3f}')\n",
        "    \n",
        "    # Pairwise\n",
        "    models = e1['red_model_short'].unique()\n",
        "    print('\\nPairwise t-tests (F1):')\n",
        "    for i, m1 in enumerate(models):\n",
        "        for m2 in models[i+1:]:\n",
        "            t, p = stats.ttest_ind(e1[e1['red_model_short']==m1]['f1_score'],\n",
        "                                    e1[e1['red_model_short']==m2]['f1_score'], equal_var=False)\n",
        "            stars = '***' if p<.001 else '**' if p<.01 else '*' if p<.05 else 'ns'\n",
        "            print(f'  {m1} vs {m2}: t={t:.2f}, p={p:.4f} {stars}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure: E1 Model Comparison\n",
        "if len(e1) > 0:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    model_order = e1.groupby('red_model_short')['f1_score'].mean().sort_values(ascending=False).index.tolist()\n",
        "    \n",
        "    sns.boxplot(data=e1, x='red_model_short', y='f1_score', order=model_order, palette='Set2', ax=axes[0], width=0.6)\n",
        "    axes[0].set_title('F1 Score by Model', fontweight='bold')\n",
        "    axes[0].set_ylabel('F1 Score'); axes[0].set_xlabel('Model')\n",
        "    axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
        "    \n",
        "    for model in model_order:\n",
        "        s = e1[e1['red_model_short']==model]\n",
        "        axes[1].scatter(s['recall'], s['precision'], alpha=0.4, label=model, s=30)\n",
        "        axes[1].scatter(s['recall'].mean(), s['precision'].mean(), marker='X', s=150, edgecolors='black', linewidths=1.5, zorder=5)\n",
        "    axes[1].set_title('Precision vs Recall', fontweight='bold')\n",
        "    axes[1].set_xlabel('Recall'); axes[1].set_ylabel('Precision'); axes[1].legend(fontsize=10)\n",
        "    axes[1].xaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
        "    axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
        "    \n",
        "    plt.suptitle('E1: Model Comparison', fontsize=15, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'e1_model_comparison.pdf')\n",
        "    plt.savefig(figures_dir / 'e1_model_comparison.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E4: Difficulty Scaling & E2: Multi-Agent Ablation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# E4: Difficulty Scaling\n",
        "e4 = next((df for k, df in experiments.items() if any(x in k.lower() for x in ['difficulty','e4','scaling'])), pd.DataFrame())\n",
        "if len(e4) > 0:\n",
        "    print(f'E4: {len(e4)} games')\n",
        "    display(e4.groupby('difficulty').agg(\n",
        "        F1=('f1_score', ['mean','std','count']),\n",
        "        Recall=('recall', ['mean','std']),\n",
        "        Precision=('precision', ['mean','std']),\n",
        "        Evasion=('evasion_rate', ['mean','std']),\n",
        "    ).round(3).reindex(['easy','medium','hard']))\n",
        "    \n",
        "    # ANOVA\n",
        "    groups = [g['f1_score'].values for d, g in e4.groupby('difficulty') if d in ['easy','medium','hard']]\n",
        "    f_stat, p_val = stats.f_oneway(*groups)\n",
        "    print(f'ANOVA (F1 ~ difficulty): F={f_stat:.3f}, p={p_val:.6f}')\n",
        "    \n",
        "    # Interaction: difficulty Ã— model\n",
        "    if 'red_model_short' in e4.columns:\n",
        "        print('\\nDifficulty Ã— Model:')\n",
        "        display(e4.pivot_table(values='f1_score', index='difficulty', columns='red_model_short', aggfunc='mean').round(3).reindex(['easy','medium','hard']))\n",
        "else:\n",
        "    print('E4 not found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# E2: Multi-Agent Ablation\n",
        "e2 = next((df for k, df in experiments.items() if any(x in k.lower() for x in ['multi','e2','ablation'])), pd.DataFrame())\n",
        "if len(e2) > 0:\n",
        "    print(f'E2: {len(e2)} games')\n",
        "    display(e2.groupby('condition').agg(\n",
        "        F1=('f1_score', ['mean','std','count']),\n",
        "        Evasion=('evasion_rate', ['mean','std']),\n",
        "        Recall=('recall', ['mean','std']),\n",
        "        Precision=('precision', ['mean','std']),\n",
        "    ).round(3))\n",
        "    \n",
        "    # vs baseline\n",
        "    if 'baseline' in e2['condition'].values:\n",
        "        bl = e2[e2['condition']=='baseline']\n",
        "        print('\\nvs Baseline (evasion rate):')\n",
        "        for c in e2['condition'].unique():\n",
        "            if c == 'baseline': continue\n",
        "            other = e2[e2['condition']==c]\n",
        "            t, p = stats.ttest_ind(bl['evasion_rate'], other['evasion_rate'], equal_var=False)\n",
        "            delta = other['evasion_rate'].mean() - bl['evasion_rate'].mean()\n",
        "            stars = '***' if p<.001 else '**' if p<.01 else '*' if p<.05 else 'ns'\n",
        "            print(f'  {c:20s}: Î”={delta:+.1%}, t={t:.2f}, p={p:.4f} {stars}')\n",
        "else:\n",
        "    print('E2 not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combined Figures (E4 + E2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure: E4 Difficulty + E2 Multi-Agent side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "if len(e4) > 0:\n",
        "    diff_order = ['easy', 'medium', 'hard']\n",
        "    if 'red_model_short' in e4.columns:\n",
        "        interaction = e4.pivot_table(values='f1_score', index='difficulty', columns='red_model_short', aggfunc='mean')\n",
        "        interaction = interaction.reindex(diff_order)\n",
        "        interaction.plot(ax=axes[0], marker='o', linewidth=2, markersize=8)\n",
        "    else:\n",
        "        sns.boxplot(data=e4, x='difficulty', y='f1_score', order=diff_order, palette=['#2ecc71','#f39c12','#e74c3c'], ax=axes[0])\n",
        "    axes[0].set_title('E4: Difficulty Ã— Model', fontweight='bold')\n",
        "    axes[0].set_ylabel('Mean F1 Score')\n",
        "    axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
        "    axes[0].legend(title='Model', fontsize=9)\n",
        "\n",
        "if len(e2) > 0:\n",
        "    cond_order = ['baseline', 'red_pipeline', 'blue_ensemble', 'full_multiagent']\n",
        "    cond_order = [c for c in cond_order if c in e2['condition'].values]\n",
        "    colors = ['#95a5a6', '#e74c3c', '#2ecc71', '#9b59b6']\n",
        "    sns.barplot(data=e2, x='condition', y='evasion_rate', order=cond_order,\n",
        "                palette=colors[:len(cond_order)], ax=axes[1], ci=95, capsize=0.1)\n",
        "    axes[1].set_title('E2: Evasion Rate by Mode', fontweight='bold')\n",
        "    axes[1].set_ylabel('Evasion Rate')\n",
        "    axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
        "    axes[1].tick_params(axis='x', rotation=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(figures_dir / 'e4_e2_combined.pdf')\n",
        "plt.savefig(figures_dir / 'e4_e2_combined.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Table & LaTeX Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paper summary\n",
        "total = sum(len(df) for df in experiments.values())\n",
        "print(f'PAPER SUMMARY: {total} games across {len(experiments)} experiments')\n",
        "print('=' * 70)\n",
        "print(f'{\"Experiment\":<30} {\"n\":>5} {\"F1\":>12} {\"Recall\":>12} {\"Evasion\":>12}')\n",
        "print('-' * 70)\n",
        "for name, df in experiments.items():\n",
        "    n = len(df)\n",
        "    print(f'{name[:28]:<30} {n:>5} '\n",
        "          f'{df[\"f1_score\"].mean():.1%}Â±{df[\"f1_score\"].std():.1%} '\n",
        "          f'{df[\"recall\"].mean():.1%}Â±{df[\"recall\"].std():.1%} '\n",
        "          f'{df[\"evasion_rate\"].mean():.1%}Â±{df[\"evasion_rate\"].std():.1%}')\n",
        "\n",
        "# Key findings\n",
        "print('\\n\\nKEY FINDINGS:')\n",
        "if len(e3) > 0:\n",
        "    db = e3[e3['condition']=='database']; nv = e3[e3['condition']=='novel']\n",
        "    if len(db)>0 and len(nv)>0:\n",
        "        print(f'  1. Novel recall={nv[\"recall\"].mean():.1%} vs Database={db[\"recall\"].mean():.1%} â†’ genuine reasoning')\n",
        "if len(e1) > 0:\n",
        "    best = e1.groupby('red_model_short')['f1_score'].mean().idxmax()\n",
        "    print(f'  2. Best model: {best}, all models recall >90% â†’ precision is bottleneck')\n",
        "if len(e4) > 0:\n",
        "    print(f'  3. Difficulty inversion: Easy F1={e4[e4[\"difficulty\"]==\"easy\"][\"f1_score\"].mean():.1%} < Hard F1={e4[e4[\"difficulty\"]==\"hard\"][\"f1_score\"].mean():.1%}')\n",
        "if len(e2) > 0:\n",
        "    bl_ev = e2[e2['condition']=='baseline']['evasion_rate'].mean() if 'baseline' in e2['condition'].values else 0\n",
        "    fm_ev = e2[e2['condition']=='full_multiagent']['evasion_rate'].mean() if 'full_multiagent' in e2['condition'].values else 0\n",
        "    print(f'  4. Arms race: Full multi-agent evasion={fm_ev:.1%} vs baseline={bl_ev:.1%}')"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
