{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§ª Run Batch Experiment\n",
        "\n",
        "This notebook runs multiple adversarial games with different configurations and analyzes the aggregated results.\n",
        "\n",
        "## Experiment Matrix\n",
        "Configure combinations of:\n",
        "- **Models**: Different LLMs for Red/Blue teams\n",
        "- **Difficulties**: easy, medium, hard\n",
        "- **Scenarios**: Different infrastructure domains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(project_root / '.env')\n",
        "\n",
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "from src.game.engine import GameEngine, GameConfig\n",
        "from src.game.scenarios import ScenarioGenerator, Scenario\n",
        "from src.agents.red_team_agent import Difficulty\n",
        "\n",
        "print(f\"âœ“ Project root: {project_root}\")\n",
        "print(\"âœ“ All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure Experiment\n",
        "\n",
        "Define the experiment matrix - all combinations will be tested:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ðŸ§ª EXPERIMENT CONFIGURATION\n",
        "# ============================================\n",
        "\n",
        "EXPERIMENT_NAME = \"notebook_experiment\"\n",
        "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
        "\n",
        "# Models to test\n",
        "MODELS = [\n",
        "    {\"id\": \"us.anthropic.claude-3-5-haiku-20241022-v1:0\", \"name\": \"Haiku\"},\n",
        "    # {\"id\": \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\", \"name\": \"Sonnet\"},  # Uncomment for more tests\n",
        "]\n",
        "\n",
        "# Difficulty levels\n",
        "DIFFICULTIES = [\"medium\", \"hard\"]\n",
        "\n",
        "# Scenarios to test\n",
        "SCENARIOS = [\n",
        "    {\"domain\": \"storage\", \"description\": \"Create S3 bucket for PHI data with security controls\"},\n",
        "    {\"domain\": \"compute\", \"description\": \"Create Lambda function processing payment data\"},\n",
        "    {\"domain\": \"iam\", \"description\": \"Create IAM roles for CI/CD pipeline\"},\n",
        "]\n",
        "\n",
        "# Calculate total games\n",
        "total_games = len(MODELS) * len(MODELS) * len(DIFFICULTIES) * len(SCENARIOS)\n",
        "print(f\"ðŸ“Š Experiment: {EXPERIMENT_NAME}\")\n",
        "print(f\"   Models: {len(MODELS)}\")\n",
        "print(f\"   Difficulties: {len(DIFFICULTIES)}\")  \n",
        "print(f\"   Scenarios: {len(SCENARIOS)}\")\n",
        "print(f\"   Total games: {total_games}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generate Game Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate all game configurations\n",
        "game_configs = []\n",
        "\n",
        "for red_model in MODELS:\n",
        "    for blue_model in MODELS:\n",
        "        for difficulty in DIFFICULTIES:\n",
        "            for scenario in SCENARIOS:\n",
        "                config = GameConfig(\n",
        "                    red_model_id=red_model[\"id\"],\n",
        "                    blue_model_id=blue_model[\"id\"],\n",
        "                    difficulty=Difficulty[difficulty.upper()],\n",
        "                    scenario=Scenario(\n",
        "                        domain=scenario[\"domain\"],\n",
        "                        description=scenario[\"description\"],\n",
        "                        cloud_provider=\"aws\",\n",
        "                        language=\"terraform\"\n",
        "                    ),\n",
        "                    region=AWS_REGION\n",
        "                )\n",
        "                game_configs.append({\n",
        "                    \"config\": config,\n",
        "                    \"red_model_name\": red_model[\"name\"],\n",
        "                    \"blue_model_name\": blue_model[\"name\"],\n",
        "                    \"difficulty\": difficulty,\n",
        "                    \"domain\": scenario[\"domain\"]\n",
        "                })\n",
        "\n",
        "print(f\"âœ“ Generated {len(game_configs)} game configurations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run Experiment\n",
        "\n",
        "âš ï¸ **This will make LLM API calls and may take several minutes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all games\n",
        "results = []\n",
        "engine = GameEngine()\n",
        "\n",
        "for i, game in enumerate(game_configs, 1):\n",
        "    print(f\"\\nðŸŽ® Running game {i}/{len(game_configs)}\")\n",
        "    print(f\"   Red: {game['red_model_name']} | Blue: {game['blue_model_name']}\")\n",
        "    print(f\"   Difficulty: {game['difficulty']} | Domain: {game['domain']}\")\n",
        "    \n",
        "    try:\n",
        "        result = await engine.play(game[\"config\"])\n",
        "        \n",
        "        results.append({\n",
        "            \"game_id\": i,\n",
        "            \"red_model\": game[\"red_model_name\"],\n",
        "            \"blue_model\": game[\"blue_model_name\"],\n",
        "            \"difficulty\": game[\"difficulty\"],\n",
        "            \"domain\": game[\"domain\"],\n",
        "            \"precision\": result.scoring.metrics.precision,\n",
        "            \"recall\": result.scoring.metrics.recall,\n",
        "            \"f1_score\": result.scoring.metrics.f1_score,\n",
        "            \"evasion_rate\": result.scoring.metrics.evasion_rate,\n",
        "            \"red_vulns\": result.scoring.counts.total_red_vulns,\n",
        "            \"blue_findings\": result.scoring.counts.total_blue_findings,\n",
        "            \"true_positives\": result.scoring.counts.true_positives,\n",
        "            \"status\": \"success\"\n",
        "        })\n",
        "        print(f\"   âœ“ F1: {result.scoring.metrics.f1_score:.1%}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        results.append({\n",
        "            \"game_id\": i,\n",
        "            \"red_model\": game[\"red_model_name\"],\n",
        "            \"blue_model\": game[\"blue_model_name\"],\n",
        "            \"difficulty\": game[\"difficulty\"],\n",
        "            \"domain\": game[\"domain\"],\n",
        "            \"status\": \"failed\",\n",
        "            \"error\": str(e)\n",
        "        })\n",
        "        print(f\"   âŒ Failed: {e}\")\n",
        "\n",
        "print(f\"\\nâœ“ Completed {len([r for r in results if r['status'] == 'success'])}/{len(game_configs)} games\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Analyze Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataFrame for analysis\n",
        "df = pd.DataFrame([r for r in results if r[\"status\"] == \"success\"])\n",
        "\n",
        "if len(df) > 0:\n",
        "    print(\"ðŸ“Š EXPERIMENT RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Overall metrics\n",
        "    print(f\"\\nðŸ“ˆ Overall Metrics (n={len(df)}):\")\n",
        "    print(f\"   Avg Precision:    {df['precision'].mean():.1%}\")\n",
        "    print(f\"   Avg Recall:       {df['recall'].mean():.1%}\")\n",
        "    print(f\"   Avg F1 Score:     {df['f1_score'].mean():.1%}\")\n",
        "    print(f\"   Avg Evasion Rate: {df['evasion_rate'].mean():.1%}\")\n",
        "    \n",
        "    # By difficulty\n",
        "    print(f\"\\nðŸ“Š By Difficulty:\")\n",
        "    difficulty_stats = df.groupby('difficulty')[['f1_score', 'evasion_rate']].mean()\n",
        "    print(difficulty_stats.to_string())\n",
        "    \n",
        "    # By domain\n",
        "    print(f\"\\nðŸ“Š By Domain:\")\n",
        "    domain_stats = df.groupby('domain')[['f1_score', 'evasion_rate']].mean()\n",
        "    print(domain_stats.to_string())\n",
        "else:\n",
        "    print(\"âŒ No successful games to analyze\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Display full results table\n",
        "if len(df) > 0:\n",
        "    display_cols = ['game_id', 'red_model', 'blue_model', 'difficulty', 'domain', \n",
        "                    'precision', 'recall', 'f1_score', 'evasion_rate']\n",
        "    print(\"\\nðŸ“‹ Full Results Table:\")\n",
        "    print(df[display_cols].to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Save Experiment Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save results\n",
        "output_dir = project_root / \"experiments\" / f\"{EXPERIMENT_NAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save as JSON\n",
        "with open(output_dir / \"results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2, default=str)\n",
        "\n",
        "# Save as CSV\n",
        "if len(df) > 0:\n",
        "    df.to_csv(output_dir / \"results.csv\", index=False)\n",
        "\n",
        "print(f\"âœ“ Results saved to: {output_dir}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
